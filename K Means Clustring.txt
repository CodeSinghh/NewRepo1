import csv
import numpy as np
import pandas as pd
mydata = pd.read_csv('/content/drive/MyDrive/ML Data/IRIS.csv')

mydata.head(2)

mydata['species'].unique()

import seaborn as sns
import matplotlib.pyplot as plt
sns.FacetGrid(mydata, hue='species', height = 3).map(sns.distplot, "petal_length").add_legend()
sns.FacetGrid(mydata, hue='species', height = 3).map(sns.distplot, "petal_width").add_legend()
sns.FacetGrid(mydata, hue='species', height = 3).map(sns.distplot, "sepal_length").add_legend()
sns.FacetGrid(mydata, hue='species', height = 3).map(sns.distplot, "sepal_width").add_legend()

plt.show()

Data Visualization

sns.set_style("whitegrid")
sns.pairplot(mydata, hue = "species", size = 3)
plt.show()

Dropping Categorical Values

x = mydata.iloc[ :, [0, 1, 2, 3]].values
mydata.head(2)

**K-Means**

from sklearn.cluster import KMeans
wcss = []
for i in range (1, 11):
  kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)
  kmeans.fit(x)
  wcss.append(kmeans.inertia_)

Using the elbow method to determine the optimal number of clusters for K-means clustering

plt.plot(range(1, 11), wcss)
plt.title('The elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

Implementing K-Means

kmeans = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)
y_kmeans = kmeans.fit_predict(x)

plt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 100, c = 'purple', label = 'Iris-setosa')
plt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 100, c = 'orange', label = 'Iris-versicolor')
plt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 100, c = 'green', label = 'Iris-virginica')

plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 100, c = 'red', label = 'Centroids')

plt.legend()

